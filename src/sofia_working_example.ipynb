{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import InceptionTime.load_data as dataloader\n",
    "import InceptionTime.InceptionModule_keras\n",
    "import InceptionTime.utils\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import random\n",
    "seed = 18\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import ucr_dataset_list\n",
    "\n",
    "# During development only choose a subset, to speed up load\n",
    "DATASET_NAMES = ucr_dataset_list()[60:80]\n",
    "print(DATASET_NAMES)\n",
    "\n",
    "datasets_dict = {}\n",
    "meta_data_dict = {'name': [], 'train_size': [], 'test_size':[] ,'length':[], 'test_proportion':[], \"num_classes\":[]}     # pandas dataframe with metadata about UCR datasets\n",
    "\n",
    "CACHED_DATA_FOLDER = os.path.dirname(os.path.dirname(os.getcwd())) + \"/Data\"\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    cache_path = os.path.join(CACHED_DATA_FOLDER, dataset_name)\n",
    "    dataset_obj = dataloader.fetch_ucr_dataset(dataset=dataset_name, use_cache=True, data_home=cache_path)\n",
    "    datasets_dict[dataset_name] = dataset_obj\n",
    "\n",
    "    # Filter the datasets depending on number of classes\n",
    "    num_classes = len(np.unique(dataset_obj['target_train']))\n",
    "    #if num_clases < 3:\n",
    "    \n",
    "    data_length = dataset_obj['data_train'].shape[1]\n",
    "    train_size = dataset_obj['data_train'].shape[0]\n",
    "    test_size = dataset_obj['data_test'].shape[0]\n",
    "    (labels,counts) = np.unique(dataset_obj['target_test'],return_counts=True)\n",
    "    test_proportion = counts[0]/(counts[0]+counts[1])\n",
    "\n",
    "    meta_data_dict['length'].append(data_length)\n",
    "    meta_data_dict['train_size'].append(train_size)\n",
    "    meta_data_dict['test_size'].append(test_size)\n",
    "    meta_data_dict['name'].append(dataset_name)\n",
    "    meta_data_dict['test_proportion'].append(test_proportion)\n",
    "    meta_data_dict['num_classes'].append(num_classes)\n",
    "\n",
    "\n",
    "meta_df = pd.DataFrame(data=meta_data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out too large datasets and split into three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reasonable_datasets = meta_df.loc[(meta_df['train_size']>200)&(meta_df['train_size']<2000)& (meta_df['test_size']<2*meta_df['train_size']) & (meta_df['length']<1000)]\n",
    "print(reasonable_datasets)\n",
    "split = int(reasonable_datasets.shape[0]/3)\n",
    "print(\"sofia:\\n\", reasonable_datasets['name'][0:split].to_string(index=False).split())\n",
    "print(\"ayman:\\n\", reasonable_datasets['name'][split:2*split].to_string(index=False).split())\n",
    "print(\"nils:\\n\", reasonable_datasets['name'][2*split:].to_string(index=False).split())\n",
    "\n",
    "print(meta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from surrogate_augmentation import refined_AAFT_surrogates, correlated_noise_surrogates, white_noise_surrogates, AAFT_surrogates\n",
    "\n",
    "\n",
    "\n",
    "def augment_data(X, aug_type):\n",
    "    n_iters = 10\n",
    "    if aug_type==\"refined_AAFT\":\n",
    "      return refined_AAFT_surrogates(X, n_iters)\n",
    "    elif aug_type==\"AAFT\":\n",
    "      return AAFT_surrogates(X)\n",
    "    elif aug_type==\"correlated_noise\":\n",
    "      correlated_noise_surrogates(X)\n",
    "    elif aug_type==\"white_noise\":\n",
    "      return white_noise_surrogates(X)\n",
    "    elif aug_type==\"no_aug\":\n",
    "      return X\n",
    "    else:\n",
    "      print(\"invalid augmentation type\")\n",
    "\n",
    "def preprocess_data(dataset_name, aug_type):\n",
    "\n",
    "    processed_data_dict = {}\n",
    "    X = augment_data(datasets_dict[dataset_name]['data_train'], aug_type)\n",
    "\n",
    "    Y = datasets_dict[dataset_name]['target_train']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.25, stratify=Y)\n",
    "    x_test = datasets_dict[dataset_name]['data_test']\n",
    "    y_test = datasets_dict[dataset_name]['target_test']\n",
    "\n",
    "    num_classes = len(np.unique(np.concatenate((y_train, y_val, y_test), axis=0)))\n",
    "\n",
    "    y_train, y_val, y_test = InceptionTime.utils.transform_labels(y_train, y_val, y_test)\n",
    "\n",
    "    # save orignal y because later we will use binary\n",
    "    y_test = y_test.astype(np.int64)\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    y_val = y_val.astype(np.int64)\n",
    "\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(np.concatenate((y_train, y_test, y_val), axis=0).reshape(-1, 1))\n",
    "    y_train_onehot = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "    y_test_onehot = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "    y_val_onehot = enc.transform(y_val.reshape(-1, 1)).toarray()\n",
    "\n",
    "    if len(x_train.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension\n",
    "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    processed_data_dict = {\"x_train\":x_train, \"y_train\":y_train, \"y_train_onehot\":y_train_onehot,\n",
    "                                \"x_test\":x_test, \"y_test\":y_test, \"y_test_onehot\":y_test_onehot,\n",
    "                                \"x_val\":x_val, \"y_val\":y_val, \"y_val_onehot\":y_val_onehot,\n",
    "                                \"num_classes\":num_classes}\n",
    "\n",
    "\n",
    "    return processed_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train on augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_EPOCHS = 1000\n",
    "N_ENSEMBLE = 1\n",
    "\n",
    "pretrained_networks = {}\n",
    "\n",
    "augmentation_types = [\"refined_AAFT\"] #[\"no_aug\", \"refined_AAFT\", \"white_noise\", \"AAFT\"]\n",
    "\n",
    "pretrain_metrics = {}\n",
    "#for name, dataset_object in processed_data_dict.items():\n",
    "\n",
    "\n",
    "for name in [\"LargeKitchenAppliances\"]:\n",
    "\n",
    "  for aug_type in augmentation_types:\n",
    "      print(aug_type)\n",
    "\n",
    "      pretrain_metrics[aug_type] = {\"loss\":[], \"recall\":[], \"precision\":[], \"accuracy\":[], \"f1_score\":[]}\n",
    "\n",
    "      pretrained_networks[aug_type] = []\n",
    "\n",
    "      for i in range(N_ENSEMBLE):\n",
    "        # Do this for each ensemble member so that we get slightly different training sets for each\n",
    "\n",
    "        processed_data = preprocess_data(name, aug_type)\n",
    "\n",
    "\n",
    "        num_classes = processed_data['num_classes']\n",
    "        input_shape = (processed_data[\"x_train\"].shape[1], 1)\n",
    "\n",
    "        verbose = True\n",
    "        use_residual = True\n",
    "        early_stopping = False\n",
    "\n",
    "        checkpoints_path = \"\"\n",
    "        inception_net = InceptionTime.InceptionModule_keras.Classifier_INCEPTION(checkpoints_path,\n",
    "                          input_shape, num_classes, save_weights=False, verbose=verbose,\n",
    "                            use_residual=use_residual, lr=0.0001, wd=1.5, early_stop=early_stopping)\n",
    "        # Do not use validation data now because we only want to train a short while, and don't use early stopping etc\n",
    "        history = inception_net.fit(processed_data['x_train'], processed_data['y_train_onehot'], processed_data['x_val'], processed_data['y_val_onehot'], nb_epochs=N_EPOCHS, plot_test_acc=True)\n",
    "\n",
    "        # Save metrics\n",
    "        pretrain_metrics[aug_type]['loss'].append(history.history['loss'])\n",
    "        pretrain_metrics[aug_type]['accuracy'].append(history.history['accuracy'])\n",
    "        pretrain_metrics[aug_type]['precision'].append(history.history['Precision'])\n",
    "        pretrain_metrics[aug_type]['recall'].append(history.history['Recall'])\n",
    "        pretrain_metrics[aug_type]['f1_score'].append(history.history['F1_score'])\n",
    "\n",
    "        pretrained_networks[aug_type].append(inception_net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_EPOCHS = 50\n",
    "N_ENSEMBLE = 1\n",
    "\n",
    "use_pretrained = True\n",
    "\n",
    "trained_networks = {}\n",
    "train_metrics = {}\n",
    "val_metrics = {}\n",
    "\n",
    "#for name, dataset_object in processed_data_dict.items():\n",
    "for name in [\"LargeKitchenAppliances\"]:\n",
    "\n",
    "   num_classes = processed_data['num_classes']\n",
    "\n",
    "   for aug_type in augmentation_types:\n",
    "\n",
    "      trained_networks[aug_type] = []\n",
    "\n",
    "      train_metrics[aug_type] = {\"loss\":[], \"recall\":[], \"precision\":[], \"accuracy\":[], \"f1_score\":[]} \n",
    "      val_metrics[aug_type] = {\"loss\":[], \"recall\":[], \"precision\":[], \"accuracy\":[], \"f1_score\":[]} \n",
    "\n",
    "      for i in range(N_ENSEMBLE):\n",
    "         print(i)\n",
    "\n",
    "         checkpoints_path = \"\"\n",
    "         processed_data = preprocess_data(name)\n",
    "         x_train=processed_data['x_train']\n",
    "         input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "         if use_pretrained:\n",
    "            inception_net = pretrained_networks[aug_type][i]\n",
    "         else:\n",
    "            inception_net = InceptionTime.InceptionModule_keras.Classifier_INCEPTION(checkpoints_path, input_shape, num_classes, save_weights=False, verbose=verbose, use_residual=use_residual)\n",
    " \n",
    "         history = inception_net.fit(x_train, processed_data['y_train_onehot'], processed_data['x_val'], processed_data['y_val_onehot'], nb_epochs=N_EPOCHS, plot_test_acc=True)\n",
    "   \n",
    "\n",
    "         # Save metrics\n",
    "         train_metrics[aug_type]['loss'].append(history.history['loss'])\n",
    "         train_metrics[aug_type]['accuracy'].append(history.history['accuracy'])\n",
    "         train_metrics[aug_type]['precision'].append(history.history['Precision'])\n",
    "         train_metrics[aug_type]['recall'].append(history.history['Recall'])\n",
    "         train_metrics[aug_type]['f1_score'].append(history.history['F1_score'])\n",
    "         val_metrics[aug_type]['loss'].append(history.history['val_loss'])\n",
    "         val_metrics[aug_type]['accuracy'].append(history.history['val_accuracy'])\n",
    "         val_metrics[aug_type]['precision'].append(history.history['val_Precision'])\n",
    "         val_metrics[aug_type]['recall'].append(history.history['val_Recall'])\n",
    "         val_metrics[aug_type]['f1_score'].append(history.history['val_F1_score'])\n",
    "\n",
    "         trained_networks[aug_type].append(inception_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "augmentation_types = [\"x_train\", \"x_refined_AAFT\"]\n",
    "\n",
    "fig, axs = plt.subplots(2, len(augmentation_types))\n",
    "\n",
    "\n",
    "for j,aug_type in enumerate(augmentation_types):\n",
    "    for i in range(N_ENSEMBLE):\n",
    "        epochs = len(train_metrics[aug_type]['loss'][i])\n",
    "        if epochs<N_EPOCHS:\n",
    "            for metric in train_metrics[aug_type].keys():\n",
    "                train_metrics[aug_type][metric][i]= np.pad(train_metrics[aug_type][metric][i], (0,N_EPOCHS-epochs), constant_values=train_metrics[aug_type][metric][i][epochs-1])\n",
    "                val_metrics[aug_type][metric][i] = np.pad(val_metrics[aug_type][metric][i], (0,N_EPOCHS-epochs), constant_values=val_metrics[aug_type][metric][i][epochs-1])\n",
    "\n",
    "\n",
    "for i in range(len(augmentation_types)):\n",
    "    aug_type=augmentation_types[i]\n",
    "    axs[0,i].plot([i for i in range(N_EPOCHS)], np.mean(train_metrics[aug_type]['loss'], axis=0) , label=\"train loss\")\n",
    "    axs[0,i].plot([i for i in range(N_EPOCHS)], np.mean(val_metrics[aug_type]['loss'],axis=0), label=\"val loss\")\n",
    "    axs[0,i].set_title(aug_type)\n",
    "    axs[1,i].plot([i for i in range(N_EPOCHS)], np.mean(val_metrics[aug_type]['accuracy'], axis=0), label=\"val acc\")\n",
    "\n",
    "axs[0,i].legend()\n",
    "axs[1,i].legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# For the whole ensemble, are the metrics just the mean of the individual networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aug_type in augmentation_types:\n",
    "    all_preds = np.zeros_like(processed_data['y_test_onehot'])\n",
    "    for i in range(N_ENSEMBLE):\n",
    "        network = trained_networks[aug_type][i]\n",
    "        prediction = network.predict(processed_data['x_test'], False)\n",
    "        all_preds += prediction/N_ENSEMBLE\n",
    "    metrics = InceptionTime.utils.calculate_metrics(processed_data['y_test'], np.argmax(all_preds, axis=1),0.0)\n",
    "    print(aug_type)\n",
    "    print(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}