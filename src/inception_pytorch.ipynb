{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHHk9Zfw35BXmyPhiKwIu7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1xronFNuigsJ"},"outputs":[],"source":["from torch import nn, Tensor, concatenate, norm, sigmoid\n","from torch.nn import ModuleList, TripletMarginLoss, BCEWithLogitsLoss\n","from torch.nn.functional import relu, normalize\n","\n","\n","class InceptionModule(nn.Module):\n","    NUM_FILTER_SETS = 3\n","\n","    def __init__(self, in_dim: int, hidden_dim: int, bottleneck_dim: int, base_kernel_size: int,\n","                 residual: bool):\n","        min_base_kernel_size = 2 ** (self.NUM_FILTER_SETS - 1)\n","        assert base_kernel_size >= min_base_kernel_size, f'base kernel size must be {min_base_kernel_size} or greater'\n","        super().__init__()\n","\n","        # The outputs from the filter sets will be concatenated feature-wise along with the parallel low pass filter.\n","        out_dim = hidden_dim * (self.NUM_FILTER_SETS + 1)\n","        filter_in_dim = in_dim\n","        if bottleneck_dim > 0 and in_dim > 1:\n","            self.bottleneck = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1, bias=False, padding='same')\n","            filter_in_dim = bottleneck_dim\n","\n","        kernel_sizes = [base_kernel_size // (2 ** i) for i in range(self.NUM_FILTER_SETS)]\n","\n","        self.filter_sets = ModuleList([\n","            nn.Conv1d(filter_in_dim, hidden_dim, kernel_size=ks, padding='same', bias=False) for ks in kernel_sizes])\n","\n","        self.bn = nn.BatchNorm1d(out_dim)\n","\n","        self.parallel_low_pass_filter = nn.Sequential(*[\n","            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n","            nn.Conv1d(in_dim, hidden_dim, kernel_size=1, padding='same', bias=False)\n","        ])\n","\n","        # Not sure if residual should be used at every module, but from the paper it doesn't seem to have a significant\n","        # effect anyway.\n","        if residual:\n","            self.residual = nn.Sequential(*[\n","                nn.Conv1d(in_dim, out_dim, kernel_size=1, bias=False, padding='same'),\n","                # Not sure if it's important that the residual has its own batch norm. Will keep it just in case.\n","                nn.BatchNorm1d(out_dim),\n","            ])\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        org_x = x\n","        if self.bottleneck is not None:\n","            x = self.bottleneck(x)\n","\n","        filter_outputs = []\n","        for filter_set in self.filter_sets:\n","            filter_outputs.append(filter_set(x))\n","\n","        filter_outputs.append(self.parallel_low_pass_filter(org_x))\n","\n","        x = concatenate(filter_outputs, dim=1)\n","        x = self.bn(x)\n","\n","        if self.residual is not None:\n","            x = x + self.residual(org_x)\n","\n","        return relu(x)\n","\n"]}]}