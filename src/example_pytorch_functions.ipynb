{"cells":[{"cell_type":"markdown","metadata":{"id":"7FB6D7FonLF8"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpPWfCBBBIzS"},"outputs":[],"source":["#!pip install python-binance\n","\n","#!pip install torchinfo\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import datetime\n","import torch\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","import pickle\n","\n","\n","\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","\n","# Locate in drive (change this to your location)\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'stock_prediction/order_book_data/'\n","\n","\n","# Import custom modules\n","import sys\n","sys.path.append(base_dir)\n","from ftx_client import FtxClient as Client\n"]},{"cell_type":"markdown","metadata":{"id":"A37Iumd7G6or"},"source":["## Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_wB5glGGg_X"},"outputs":[],"source":["## DATA AUGMENTATION --------------------------------------\n","\n","def correlated_noise_surrogates(original_data):\n","    \"\"\"\n","    Return Fourier surrogates.\n","\n","    Generate surrogates by Fourier transforming the :attr:`original_data`\n","    time series (assumed to be real valued), randomizing the phases and\n","    then applying an inverse Fourier transform. Correlated noise surrogates\n","    share their power spectrum and autocorrelation function with the\n","    original_data time series.\n","\n","    The Fast Fourier transforms of all time series are cached to facilitate\n","    a faster generation of several surrogates for each time series. Hence,\n","    :meth:`clear_cache` has to be called before generating surrogates from\n","    a different set of time series!\n","\n","    .. note::\n","        The amplitudes are not adjusted here, i.e., the\n","        individual amplitude distributions are not conserved!\n","\n","    **Examples:**\n","\n","    The power spectrum is conserved up to small numerical deviations:\n","\n","    >>> ts = Surrogates.SmallTestData().original_data\n","    >>> surrogates = Surrogates.\\\n","            SmallTestData().correlated_noise_surrogates(ts)\n","    >>> all(r(np.abs(np.fft.fft(ts,         axis=1))[0,1:10]) == \\\n","            r(np.abs(np.fft.fft(surrogates, axis=1))[0,1:10]))\n","    True\n","\n","    However, the time series amplitude distributions differ:\n","\n","    >>> all(np.histogram(ts[0,:])[0] == np.histogram(surrogates[0,:])[0])\n","    False\n","\n","    :type original_data: 2D array [index, time]\n","    :arg original_data: The original time series.\n","    :rtype: 2D array [index, time]\n","    :return: The surrogate time series.\n","    \"\"\"\n","\n","    #  Calculate FFT of original_data time series\n","    #  The FFT of the original_data data has to be calculated only once,\n","    #  so it is stored in self._original_data_fft.\n","    surrogates = np.fft.rfft(original_data, axis=1)\n","\n","    #  Get shapes\n","    (N, n_time) = original_data.shape\n","    len_phase = surrogates.shape[1]\n","\n","    #  Generate random phases uniformly distributed in the\n","    #  interval [0, 2*Pi]\n","    phases = np.random.uniform(low=0, high=2 * np.pi, size=(N, len_phase))\n","\n","    #  Add random phases uniformly distributed in the interval [0, 2*Pi]\n","    surrogates *= np.exp(1j * phases)\n","\n","    #  Calculate IFFT and take the real part, the remaining imaginary part\n","    #  is due to numerical errors.\n","    return np.ascontiguousarray(np.real(np.fft.irfft(surrogates, n=n_time,\n","                                                        axis=1)))\n","\n","def AAFT_surrogates(original_data):\n","    \"\"\"\n","    Return surrogates using the amplitude adjusted Fourier transform\n","    method.\n","\n","    Reference: [Schreiber2000]_\n","\n","    :type original_data: 2D array [index, time]\n","    :arg original_data: The original time series.\n","    :rtype: 2D array [index, time]\n","    :return: The surrogate time series.\n","    \"\"\"\n","    #  Create sorted Gaussian reference series\n","    gaussian = np.random.randn(original_data.shape[0], original_data.shape[1])\n","    gaussian.sort(axis=1)\n","\n","    #  Rescale data to Gaussian distribution\n","    ranks = original_data.argsort(axis=1).argsort(axis=1)\n","    rescaled_data = np.zeros(original_data.shape)\n","\n","    for i in range(original_data.shape[0]):\n","        rescaled_data[i, :] = gaussian[i, ranks[i, :]]\n","\n","    #  Phase randomize rescaled data\n","    phase_randomized_data = correlated_noise_surrogates(rescaled_data)\n","\n","    #  Rescale back to amplitude distribution of original data\n","    sorted_original = original_data.copy()\n","    sorted_original.sort(axis=1)\n","\n","    ranks = phase_randomized_data.argsort(axis=1).argsort(axis=1)\n","\n","    for i in range(original_data.shape[0]):\n","        rescaled_data[i, :] = sorted_original[i, ranks[i, :]]\n","\n","    return rescaled_data\n","\n","def refined_AAFT_surrogates(original_data, n_iterations, output=\"true_amplitudes\"):\n","    \"\"\"\n","    Return surrogates using the iteratively refined amplitude adjusted\n","    Fourier transform method.\n","\n","    A set of AAFT surrogates (:meth:`AAFT_surrogates`) is iteratively\n","    refined to produce a closer match of both amplitude distribution and\n","    power spectrum of surrogate and original data.\n","\n","    Reference: [Schreiber2000]_\n","\n","    :type original_data: 2D array [index, time]\n","    :arg original_data: The original time series.\n","    :arg int n_iterations: Number of iterations / refinement steps\n","    :arg str output: Type of surrogate to return. \"true_amplitudes\":\n","        surrogates with correct amplitude distribution, \"true_spectrum\":\n","        surrogates with correct power spectrum, \"both\": return both outputs\n","        of the algorithm.\n","    :rtype: 2D array [index, time]\n","    :return: The surrogate time series.\n","    \"\"\"\n","    #  Get size of dimensions\n","    n_time = original_data.shape[1]\n","\n","    #  Get Fourier transform of original data with caching\n","    fourier_transform = np.fft.rfft(original_data, axis=1)\n","\n","    #  Get Fourier amplitudes\n","    original_fourier_amps = np.abs(fourier_transform)\n","\n","    #  Get sorted copy of original data\n","    sorted_original = original_data.copy()\n","    sorted_original.sort(axis=1)\n","\n","    #  Get starting point / initial conditions for R surrogates\n","    # (see [Schreiber2000]_)\n","    R = AAFT_surrogates(original_data)\n","\n","    #  Start iteration\n","    for i in range(n_iterations):\n","        #  Get Fourier phases of R surrogate\n","        r_fft = np.fft.rfft(R, axis=1)\n","        # r_phases = r_fft / np.abs(r_fft)\n","        a  = r_fft\n","        b = np.abs(r_fft)\n","        r_phases = np.divide(a, b, out = np.zeros(a.shape, dtype=np.clongdouble), where=b!=0)\n","        #  Transform back, replacing the actual amplitudes by the desired\n","        #  ones, but keeping the phases exp(iÏˆ(i)\n","        s = np.fft.irfft(original_fourier_amps * r_phases, n=n_time,\n","                            axis=1)\n","\n","        #  Rescale to desired amplitude distribution\n","        ranks = s.argsort(axis=1).argsort(axis=1)\n","\n","        for j in range(original_data.shape[0]):\n","            R[j, :] = sorted_original[j, ranks[j, :]]\n","\n","    if output == \"true_amplitudes\":\n","        return R\n","    elif output == \"true_spectrum\":\n","        return s\n","    elif output == \"both\":\n","        return (R, s)\n","    else:\n","        return (R, s)\n","\n","\n","\n","class DataTransformer:\n","    def __init__(self, mix_coef = np.pi/6, num_iter = 2):\n","        self.mix_coef = mix_coef\n","        self.num_iter = num_iter\n","\n","    def transform(self, x):\n","        transposed_data = x.T\n","        surrogated_data = refined_AAFT_surrogates(transposed_data,n_iterations=self.num_iter, output='true_spectrum')\n","        new_data = np.cos(self.mix_coef)*transposed_data + np.sin(self.mix_coef)*surrogated_data\n","        return new_data.T\n","\n","\n","class TimeseriesDataset(torch.utils.data.Dataset):\n","    def __init__(self, X, y, pytorch_test_dataset_dict, scale_proportion = 0.2):\n","        self.X = X\n","        self.y = y\n","        self.seq_len = pytorch_test_dataset_dict['seq_len']\n","        self.skip_step = pytorch_test_dataset_dict['skip_step']\n","        self.transformer = DataTransformer()\n","        self.transform_data = pytorch_test_dataset_dict['transform_data']\n","        self.is_regression = pytorch_test_dataset_dict['is_regression']\n","        self.scaling = pytorch_test_dataset_dict['scaling']\n","        self.scale_proportion = scale_proportion\n","\n","    def __len__(self):\n","        return self.X.__len__() - (self.seq_len)*self.skip_step\n","\n","    def __getitem__(self, index):\n","\n","        arr = np.asarray(self.X[index:index+(self.seq_len*self.skip_step)],dtype=np.float32)\n","        instance  = arr.reshape(-1, self.skip_step, arr.shape[1]).mean(axis = 1)\n","        target = np.asarray(self.y[(index+(self.seq_len)*self.skip_step)-1],dtype=np.float32)\n","\n","        if self.transform_data == 1:\n","            instance = self.transformer.transform(instance)\n","\n","        if self.scaling == 1:\n","            scaling_factor = np.random.uniform(1.0-self.scale_proportion, 1.0+self.scale_proportion)\n","            instance = instance * scaling_factor\n","            if self.is_regression == 1:\n","                target = target * scaling_factor\n","\n","        return (np.asarray(instance,dtype=np.float32),np.asarray(target,dtype=np.float32))\n","\n","class TimeDistributed(torch.nn.Module):\n","    def __init__(self, module, batch_first=True):\n","        super(TimeDistributed, self).__init__()\n","        self.module = module\n","        self.batch_first = batch_first\n","\n","    def forward(self, x):\n","\n","        if len(x.size()) <= 2:\n","            return self.module(x)\n","\n","        # Squash samples and timesteps into a single axis\n","        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n","\n","        y = self.module(x_reshape)\n","\n","        # We have to reshape Y\n","        if self.batch_first:\n","            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n","        else:\n","            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n","\n","        return y\n","\n","class LSTMNet(torch.nn.Module):\n","    def __init__(self, model_parameters_dict):\n","        super(LSTMNet, self).__init__()\n","\n","        self.input_features = model_parameters_dict['input_features']\n","        self.output_size = model_parameters_dict['output_size']\n","        self.embed_dim = model_parameters_dict['embed_dim']\n","        self.hidden_dim = model_parameters_dict['hidden_dim']\n","        self.n_layers = model_parameters_dict['n_layers']\n","        self.device = model_parameters_dict['device']\n","        self.drop_prob = model_parameters_dict['drop_prob']\n","        self.regression = model_parameters_dict['regression']\n","\n","        self.embedding = TimeDistributed(torch.nn.Linear(self.input_features, self.embed_dim))\n","        self.lstm = torch.nn.LSTM(self.embed_dim, self.hidden_dim, self.n_layers, dropout=self.drop_prob, batch_first=True)\n","        self.dropout = torch.nn.Dropout(self.drop_prob)\n","\n","        # DESDE: ESTAS LINEAS CAMBIARON <----------------------------------------------\n","        self.fc = torch.nn.Linear(self.hidden_dim*self.n_layers, self.embed_dim)\n","        # HASTA: ESTAS LINEAS CAMBIARON <----------------------------------------------\n","\n","        self.fc_out = torch.nn.Linear(self.embed_dim, self.output_size)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        embeds = self.embedding(x)\n","        # embeds = self.sigmoid(embeds)\n","\n","        # DESDE: ESTAS LINEAS CAMBIARON <----------------------------------------------\n","        lstm_out, (hidden,cell) = self.lstm(embeds)\n","        lstm_out = hidden.contiguous().view(batch_size, self.hidden_dim*self.n_layers) # ANTES: lstm_out[:,-1,:].contiguous().view(-1, self.hidden_dim)\n","        # HASTA: ESTAS LINEAS CAMBIARON <----------------------------------------------\n","\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        out = self.sigmoid(out)\n","        out = self.fc_out(out)\n","\n","        if self.regression == False:\n","            out = self.sigmoid(out)\n","\n","        out = out.view(batch_size, -1)\n","        out = out[:, -1]\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device),\n","                    weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(self.device))\n","        return hidden\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3X9zwQv6Ghti"},"outputs":[],"source":["\n","\n","# FUNCTION TO CREATE DATASET ---------------------------------------------------\n","\n","def create_dataset(data_candle_filtered, filtered_data_ob, dataset_param):\n","\n","    pred_steps = dataset_param['pred_steps']\n","    train_ratio = dataset_param['train_ratio']\n","    scaling_log_returns = dataset_param['scaling_log_returns']\n","    regression = dataset_param['regression']\n","    target_folder = dataset_param['target_folder']\n","    file_prefix = dataset_param['file_prefix']\n","\n","    merged_dataset = pd.merge(data_candle_filtered, filtered_data_ob, on=\"timestamp\", how='inner')\n","\n","    log_returns_close_pred = np.convolve([1]*pred_steps, merged_dataset.log_returns_close.values[1:],mode='valid')\n","    big_dataset = merged_dataset.iloc[:-pred_steps].copy()\n","\n","    big_dataset['y'] = log_returns_close_pred\n","\n","    big_dataset.dropna(inplace = True)\n","\n","    data_shape = big_dataset.shape\n","    big_dataset['train'] = False\n","    big_dataset.loc[:int(data_shape[0]*train_ratio), 'train'] = True\n","\n","    X_train = big_dataset[big_dataset.train].drop(columns = ['timestamp', 'train','y'])\n","    X_test = big_dataset[~big_dataset.train].drop(columns = ['timestamp', 'train','y'])\n","\n","    #if regression == True:\n","    if True:\n","        close_returns_values_train =  X_train['log_returns_close'].copy().values * scaling_log_returns\n","        close_returns_values_test = X_test['log_returns_close'].copy().values * scaling_log_returns\n","\n","        X_train.drop(columns='log_returns_close',inplace=True)\n","        X_test.drop(columns='log_returns_close',inplace=True)\n","\n","    y_train = np.asarray(big_dataset[big_dataset.train]['y'].values)\n","    y_test = np.asarray(big_dataset[~big_dataset.train]['y'].values)\n","\n","    feature_list = X_train.columns\n","    feature_list = list(feature_list.values)\n","\n","    ss = StandardScaler()\n","    #ss = MinMaxScaler(feature_range=(-1, 1))\n","    X_train = ss.fit_transform(X_train.values)\n","    X_test = ss.transform(X_test.values)\n","    #sacaler_parameters = ss.get_params()\n","    pickle.dump(ss, open(target_folder + file_prefix + 'std_scaler.pkl','wb'))\n","\n","    #if regression == True:\n","    if True:\n","        X_train = np.concatenate((X_train, close_returns_values_train.reshape(-1,1)), axis=1)\n","        X_test = np.concatenate((X_test, close_returns_values_test.reshape(-1,1)), axis=1)\n","        feature_list.append('log_returns_close')\n","\n","    # PRUEBAAA\n","    #X_test = X_test[int(data_shape[0]*0.1466):]\n","    #y_test = y_test[int(data_shape[0]*0.1466):]\n","\n","    return X_train, X_test, y_train, y_test, feature_list, big_dataset"]},{"cell_type":"markdown","metadata":{"id":"EWvWMUCMkGJI"},"source":["## Create the dataset and dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"executionInfo":{"elapsed":1549,"status":"ok","timestamp":1660682641440,"user":{"displayName":"Gonzalo Uribarri","userId":"17825563747069699654"},"user_tz":-120},"id":"Q_QQ2pEoA3-6","outputId":"14b32cbd-7622-4a14-dbf0-cd7e2b6a58b9"},"outputs":[],"source":["# CREATING DATASET\n","\n","X_train, X_test, y_train, y_test, feature_list, big_dataset = create_dataset(data_candle_filtered, filtered_data_ob, dataset_param_dict)\n","\n","# CREATING DATALOADERS\n","\n","train_dataset = TimeseriesDataset(X_train, y_train, pytorch_train_dataset_dict)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last=True, num_workers = False)\n","\n","test_dataset = TimeseriesDataset(X_test, y_test, pytorch_test_dataset_dict)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, drop_last=True, num_workers = False)\n","\n","\n","# PRINT DATASET STATISTICS\n","\n","bool_y_train = y_train > 0\n","proportion_train = np.mean(bool_y_train)\n","baseline_train = 100*np.max([proportion_train,1-proportion_train])\n","if proportion_train > 1-proportion_train:\n","    label_train = 'Up'\n","else:\n","    label_train = 'Down'\n","print('Baseline Train: ',baseline_train)\n","print('Mayority Train: '+ label_train)\n","\n","bool_y_test = y_test > 0\n","proportion_test = np.mean(bool_y_test)\n","baseline_test = 100*np.max([proportion_test,1-proportion_test])\n","if proportion_test > 1-proportion_test:\n","    label_test = 'Up'\n","else:\n","    label_test = 'Down'\n","\n","print('Baseline Test: ',baseline_test)\n","print('Mayority Test: '+ label_test)\n","\n","\n","# PRINT RANDOM INSTANCE\n","\n","num_ins = 100\n","ins = train_dataset.__getitem__(num_ins)[0]\n","\n","plt.figure(figsize=(8,6))\n","for i in range(len(feature_list)):\n","    plt.plot(ins[:,i],alpha=0.5, label = feature_list[i], linewidth=5.0)\n","plt.legend()\n","plt.show()\n","\n","num_intances_train = X_train.shape[0]\n","total_length_intances = SEQ_LEN * SKIP_STEP\n","num_independent_intances = int(num_intances_train/total_length_intances)\n","compute_loss_step = 100 * int(num_intances_train/(BATCH_SIZE * num_independent_intances))\n","\n","print('Number of intances:',num_intances_train)\n","print('Number independent intances:',num_independent_intances)\n","print('Batch Size:', BATCH_SIZE)\n","print('compute loss every:',compute_loss_step)\n","\n","print('Batch in training:',int(num_intances_train/BATCH_SIZE))"]},{"cell_type":"markdown","metadata":{"id":"mj3xWw58kVlF"},"source":["## Define model and optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":593},"executionInfo":{"elapsed":3703,"status":"ok","timestamp":1660662033046,"user":{"displayName":"Gonzalo Uribarri","userId":"17825563747069699654"},"user_tz":-120},"id":"6gJBCW9OFJNU","outputId":"ff8ec86c-7096-4de4-916d-1ae046faec04"},"outputs":[],"source":["list_pred_labels_train = []\n","list_real_labels_train  = []\n","\n","for inp, lab in train_loader:\n","    train_h = best_model.init_hidden(BATCH_SIZE)\n","    train_h = tuple([each.data for each in train_h])\n","\n","    inp, lab = inp.to(device), lab.to(device)\n","    out, train_h = best_model(inp)\n","\n","    pred_labels = out.squeeze().detach().cpu().numpy()\n","    real_labels = lab.float().detach().cpu().numpy()\n","\n","    list_pred_labels_train.append(pred_labels)\n","    list_real_labels_train.append(real_labels)\n","\n","vec_pred_train = np.hstack(list_pred_labels_train)\n","vec_real_train = np.hstack(list_real_labels_train)\n","\n","plt.scatter(vec_real_train,vec_pred_train)\n","plt.title('Training set')\n","plt.ylabel('Predicted Value')\n","plt.xlabel('Real Value')\n","\n","max_x = np.max(vec_pred_train)\n","min_x = np.min(vec_pred_train)\n","plt.plot([min_x,max_x],[min_x,max_x],linewidth=2,color='C1')\n","plt.show()\n","\n","corr, p = spearmanr(vec_real_train, vec_pred_train)\n","print('Coeficiente de Correlacion: ', corr)\n","print('P value: ', p)\n","\n","plt.hist([vec_real_train, vec_pred_train],bins = 20, label=['Real Values', 'Predicted Values'])\n","plt.xlabel('Log return in 60 minutes')\n","plt.legend(loc='upper left')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1pYQ7Vut9_oinkWLeE3kXGnDN3HuKTGeF","timestamp":1687188864566}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":0}
